{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab19aee-5cfe-4a95-9a34-002d539ddb13",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graph (DAG), Lazy Evaluation and Jobs in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16607d0c-aa01-4979-915c-3c588c392856",
   "metadata": {},
   "source": [
    "#### A DAG in Spark is a representation of the logical execution plan of a Spark application. \n",
    "#### It is a graph where nodes represent RDDs (or DataFrames/Datasets) and the operations (transformations) applied to them, \n",
    "#### while edges represent the flow of data and dependencies between these operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2d82d-f6df-4789-a5e7-9d70371a11ef",
   "metadata": {},
   "source": [
    "## RDD(Resilient Distributed Dataset)in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074a720-e3fe-41d3-9e24-a1dd52262fb7",
   "metadata": {},
   "source": [
    "#### RDD, or Resilient Distributed Dataset, is the fundamental data structure in Apache Spark. \n",
    "#### It represents an immutable, fault-tolerant, and distributed collection of elements that can be processed in parallel across a cluster of machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fd8c3-1ebe-4b5e-909a-c51ac9ba9d5b",
   "metadata": {},
   "source": [
    "## Parquet File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1337cc-14f1-480b-aab7-8fe9e092c766",
   "metadata": {},
   "source": [
    "#### Columnar Storage: Unlike traditional row-based formats (like CSV or JSON), Parquet stores data in columns. This organization allows for:\n",
    "#### Efficient Querying: When a query only needs specific columns, only those columns are read, significantly reducing I/O operations and improving query performance.\n",
    "#### Better Compression: Similar data types are grouped together in columns, leading to more effective compression algorithms and reduced storage space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac7b85-f665-4de6-b5a4-061c85a24444",
   "metadata": {},
   "source": [
    "## Writing DataFrames in Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00877324-eb92-413f-a2df-387d3b9d809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef93b5b-80d7-41db-83e8-f47f544fa503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 23:32:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \\\n",
    "        .appName(\"Partitioning and Bucketing\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "010fa777-69ab-4d91-a8dc-69a110eaa72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .load(\"data_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "205e1092-72ac-4caf-b869-eb367e0f7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n",
      "| id|    name|age|salary|address|gender|\n",
      "+---+--------+---+------+-------+------+\n",
      "|  1|  Manish| 26| 75000|  INDIA|     m|\n",
      "|  2|  Nikita| 23|100000|    USA|     f|\n",
      "|  3|  Pritam| 22|150000|  INDIA|     m|\n",
      "|  4|Prantosh| 17|200000|  JAPAN|     m|\n",
      "|  5|  Vikash| 31|300000|    USA|     m|\n",
      "|  6|   Rahul| 55|300000|  INDIA|     m|\n",
      "|  7|    Raju| 67|540000|    USA|     m|\n",
      "|  8| Praveen| 28| 70000|  JAPAN|     m|\n",
      "|  9|     Dev| 32|150000|  JAPAN|     m|\n",
      "| 10|  Sherin| 16| 25000| RUSSIA|     f|\n",
      "| 11|    Ragu| 12| 35000|  INDIA|     f|\n",
      "| 12|   Sweta| 43|200000|  INDIA|     f|\n",
      "| 13| Raushan| 48|650000|    USA|     m|\n",
      "| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n",
      "| 15| Prakash| 52|750000|  INDIA|     m|\n",
      "+---+--------+---+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf092b2-f422-4ed6-823b-30918deea172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
