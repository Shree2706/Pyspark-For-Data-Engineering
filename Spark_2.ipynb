{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6703e3a-d071-4f52-b036-374486416a19",
   "metadata": {},
   "source": [
    "## Handling corrupted records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dde52e9-ca6e-4b4a-91f3-177d7a370d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705a41b3-538b-4e52-b9e1-7b5f4168d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/03 23:48:43 WARN Utils: Your hostname, Shrees-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.28.47.103 instead (on interface en0)\n",
      "25/11/03 23:48:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/03 23:48:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/03 23:48:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \\\n",
    "        .appName(\"Handling corrupted records\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca156f99-d86c-44d0-a9b0-675bff3b1426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.28.47.103:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handling corrupted records</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12706fd40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a66258-cb1a-4f6c-bab4-c0cabb7784d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50dcf381-535f-484b-ac21-36f7ab6411f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|\n",
      "|  5|  Vikash| 31|300000|        NULL| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67209cf2-cb14-4188-9fc1-9a68fdbb1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d7c346-ab28-4c9b-b471-de6a5829bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType(\n",
    "                            [StructField(\"id\",IntegerType(),True),\n",
    "                             StructField(\"name\",StringType(),True),\n",
    "                             StructField(\"age\",IntegerType(),True),\n",
    "                             StructField(\"salary\",IntegerType(),True),\n",
    "                             StructField(\"address\",StringType(),True),\n",
    "                             StructField(\"nominee\",StringType(),True),\n",
    "                             StructField(\"corrupt_record\",StringType(),True)\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9dd3fbb-9ec5-44a0-8cba-45d550f6afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .schema(data_schema)\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb415f76-6a31-42fb-ac36-14e6946a519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------+\n",
      "| id|    name|age|salary|     address| nominee|corrupt_record|\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|          NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|          NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|      nominee3|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|      nominee4|\n",
      "|  5|  Vikash| 31|300000|        NULL| nominee|          NULL|\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 23:48:48 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 6, schema size: 7\n",
      "CSV file: file:///Users/shreejhariya/iCloud_Drive_(Archive)/Documents/Workspace/Pyspark/data.csv\n"
     ]
    }
   ],
   "source": [
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ff0ef3f-a23a-439b-adf1-18d2d27d6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving corrupted data (bad_records) to different file location\n",
    "data_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .schema(data_schema)\\\n",
    "    .option(\"badRecordsPath\",\"/iCloud_Drive_(Archive)/Documents/Workspace/bad_records\")\\\n",
    "    .load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0580048-f231-44ee-9ab7-0bd6ee9ea4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+--------------+\n",
      "| id|    name|age|salary|     address| nominee|corrupt_record|\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|          NULL|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|          NULL|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|      nominee3|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|      nominee4|\n",
      "|  5|  Vikash| 31|300000|        NULL| nominee|          NULL|\n",
      "+---+--------+---+------+------------+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 23:52:57 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 6, schema size: 7\n",
      "CSV file: file:///Users/shreejhariya/iCloud_Drive_(Archive)/Documents/Workspace/Pyspark/data.csv\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a5ead30-5c81-48b5-a37f-6a401497d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: /iCloud_Drive_(Archive)/Documents/Workspace/Pyspark/bad_records\n"
     ]
    }
   ],
   "source": [
    "%ls /iCloud_Drive_(Archive)/Documents/Workspace/Pyspark/bad_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2903f-2059-4de6-9f85-0f427c773924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
